---
title: "CIS635-Project"
author: "Tyler Reed"
date: "4/19/2021"
output:
  github_document: default
  pdf_document: default
---

```{r setup, message=FALSE}
knitr::opts_chunk$set(error = TRUE, fig.width = 12, fig.asp = 0.618)
```

```{r load packages, import data, warning = FALSE, message = FALSE}

library(tidyverse)
library(knitr)
library(e1071)
library(rpart)
library(rpart.plot)
library(neuralnet)
library(hrbrthemes)
library(readr)
library(purrr)
library(ggthemes)
library(varhandle)
library(fastDummies)

testA <- read.table("data/dataTestA.txt", header = TRUE)
testB <- read.table("data/dataTestB.txt", header = TRUE)
trainA <- read.table("data/dataTrainA.txt", header = TRUE)
trainB <- read.table("data/dataTrainB.txt", header = TRUE)
```


```{r preliminary analysis, A-train files, warning = FALSE, message = FALSE}

# PRELIMINARY ANALYSIS AND CLEANUP, NO GRAPHS, AND MERGE


trainA <- as_tibble(trainA)

# Calculate summary statistics and produce visuals to check for outliers/noise/NAs
trainA %>%
  summary() %>%
  kable()

# trainA  %>%
#   mutate(across(.cols = everything(), as_factor)) %>%
#   keep(is.numeric) %>% 
#   gather() %>% 
#   ggplot(aes(value)) +
#     facet_wrap(~ key, scales = "free") +
#     theme_tufte(base_size = 16) +
#     geom_histogram(color = "royalblue", bins = 500)

# Test for duplicate records
length(unique(trainA$id)) == nrow(trainA)

# Test for missing values by row
train_A_byrow<- rowSums(is.na(trainA))
max(train_A_byrow)

# Results

# No more than one NA per dataset

# id: looks good and no duplicates
# temp: 1 NA, and min and max troublesome, use average
# bbSys: 1 NA, and min and max troublesome, use average
# vo2: 2 NA, max troublesome
# throat: 1 NA, max troublesome
# atRisk: looks good

```


```{r preliminary analysis, B-train files, warning = FALSE, message = FALSE}

trainB <- as_tibble(trainB)

# Calculate summary statistics and produce visuals to check for outliers/noise/NAs
trainB %>%
  summary(trainB) %>%
  kable()
# 
# trainA %>%
#   mutate(across(.cols = everything(), as_factor)) %>%
#   select(-id) %>%
#   filter(temp > 106) %>%
#   ggplot(aes(x = temp)) +
#     geom_bar(fill = "royalblue", position = "dodge") +
#     scale_fill_brewer(palette = "Dark2") +
#     theme_tufte(base_size = 16) 
    


# Test for duplicate records
length(unique(trainB$id)) == nrow(trainB)

# Test for missing values by row
train_B_byrow <- rowSums(is.na(trainB))
max(train_B_byrow)

# Results

# id: looks good and no duplicates
# headA: 1 NA, max troublesome
# bodyA: looks good
# cough: looks good
# runny: 1 NA
# nausea: max is troublesome
# diarrhea: 1 NA
# atRisk: looks good

```

```{r cleaning helper functions, include = FALSE}

# Create helper functions to define thresholds for vitals variables

getmode <- function(v) {
  uniqv <- unique(v)
  uniqv[which.max(tabulate(match(v, uniqv)))]
}

temp <- function(v) {
  for (i in 1:length(v)) {
    if (!(between(v[i], 90, 106))) {
      v[i] <- mean(v)
    }
  }
  v
}

bpSys <- function(v) {
  for (i in 1:length(v)) {
    if (!(between(v[i], 90, 150))) {
      v[i] <- mean(v)
    }
  }
  v
}

vo2 <- function(v) {
  for (i in 1:length(v)) {
    if (!(between(v[i], 10, 70))) {
      v[i] <- mean(v)
    }
  }
  v
}

throat <- function(v) {
  for (i in 1:length(v)) {
    if (!(between(v[i], 80, 120))) {
      v[i] <- mean(v)
    }
  }
  v
}

headA<- function(v) {
  for (i in 1:length(v)) {
    if (!(between(v[i], 0, 9))) {
      v[i] <- getmode(v)
    }
  }
  v
}

bodyA <- function(v) {
  for (i in 1:length(v)) {
    if (!(between(v[i], 0, 9))) {
      v[i] <- getmode(v)
    }
  }
  v
}

cough <- function(v) {
  for (i in 1:length(v)) {
    if (v[i] != 1 & v[i] != 0) {
      v[i] <- getmode(v)
    }
  }
  v
}

runny <- function(v) {
  for (i in 1:length(v)) {
    if (v[i] != 1 & v[i] != 0) {
      v[i] <- getmode(v)
    }
  }
  v
}

nausea <- function(v) {
  for (i in 1:length(v)) {
    if (v[i] != 1 & v[i] != 0) {
      v[i] <- getmode(v)
    }
  }
  v
}

diarrhea <- function(v) {
  for (i in 1:length(v)) {
    if (v[i] != 1 & v[i] != 0) {
      v[i] <- getmode(v)
    }
  }
  v
}

# Vector of cleaning helper functions 
clean_helpers <- list(temp,
                   bpSys,
                   vo2,
                   throat,
                   headA,
                   bodyA,
                   cough,
                   runny,
                   nausea,
                   diarrhea)

# The following is to aid in testing the helper functions above
# max(temp_noise(merged_train$temp))
# min(temp_noise(merged_train$temp))
# 
# max(bpSys_noise(merged_train$bpSys))
# min(bpSys_noise(merged_train$bpSys))
# 
# max(vo2_noise(merged_train$vo2))
# min(vo2_noise(merged_train$vo2))
# 
# max(throat_noise(merged_train$throat))
# min(throat_noise(merged_train$throat))
# 
# max(headA_noise(merged_train$headA))
# min(headA_noise(merged_train$headA))
# 
# max(bodyA_noise(merged_train$bodyA))
# min(bodyA_noise(merged_train$bodyA))
# 
# max(cough_noise(merged_train$cough))
# min(cough_noise(merged_train$cough))
# 
# max(runny_noise(merged_train$runny))
# min(runny_noise(merged_train$runny))
# 
# max(nausea_noise(merged_train$nausea))
# min(nausea_noise(merged_train$nausea))
# 
# max(diarrhea_noise(merged_train$diarrhea))
# min(diarrhea_noise(merged_train$diarrhea))
```

```{r cleanup outliers/missing data and merge, warning = FALSE, message = FALSE}

clean_train <- function(A, B) {
  
  # Merge `trainA` and `trainB`
  merged_train <- A %>%
    select(-atRisk) %>%
    left_join(B, by = "id") %>%
    # Convert NAs of factor variables to the variable mode 
    mutate(across(6:12, ~ replace_na(., getmode(.)))) %>%
    mutate(across(2:5, as.numeric)) %>%
    # Convert NAs of numeric variables to the variable mean 
    mutate(across(2:5, ~ replace_na(., mean(., na.rm = TRUE))))

  # Clean data: replacing any noise with mode or mean according to type
  for (i in 2:11) {
      merged_train[, i] <- modify(merged_train[, i], clean_helpers[i - 1])
  }
  
  # Convert variables to respective types
  merged_train <- merged_train %>%
    mutate(across(6:12, as_factor))
  
  merged_train
}

xTrain <- clean_train(trainA, trainB)

kable(summary(xTrain), caption = "New Summary Statistics to Confirm Cleaned Training Data")

# Convert datatypes of variables and merge test data; testA and testB
xTest<- testA %>%
    as_tibble() %>%
    select(-atRisk) %>%
    left_join(testB, by = "id") %>%
    mutate(across(2:5, as.numeric)) %>%
    mutate(across(6:12, as_factor))
```


```{r compare classifiers, warning = FALSE, message = FALSE}



```


```{r trees, warning = FALSE, message = FALSE}

modTree <- rpart(atRisk~temp+bpSys+vo2+throat+headA+bodyA+cough+runny+nausea+diarrhea, xTrain)

rpart.plot(modTree)

predTree <- predict(modTree, xTest, type = "vector")
table(predTree, xTest$atRisk)

# 85.92% Accuracy
# 85.15% Recall
```

```{r naive bayes, warning = FALSE, message = FALSE}

modBayes <- naiveBayes(atRisk~.-id, xTrain)
predBayes <- predict(modBayes, xTest)
table(predBayes, xTest$atRisk)

# 84.67% Accuracy
# 84.5% Recall
```

```{r svm, warning = FALSE, message = FALSE}

xTrain_noFactors <- xTrain %>%
  mutate(across(where(is.factor), unfactor))

xTest_noFactors <- xTest %>%
  mutate(across(where(is.factor), unfactor))

modSVM <- svm(xTrain_noFactors[, 2:11], kernel = "linear")
predSVM <- predict(modSVM, xTest_noFactors[, 2:11])
table(predSVM, xTest_noFactors$atRisk)

# 38.61% Accuracy
# 29.75% Recall

modSVM_Poly <- svm(xTrain_noFactors[, 2:11], kernel = "polynomial")
predSVM_Poly <- predict(modSVM_Poly, xTest_noFactors[, 2:11])
table(predSVM_Poly, xTest_noFactors$atRisk)

# 63.67% Accuracy
# 60.32% Recall

```

```{r normalize data for ann, warning = FALSE, message = FALSE}

normalize <- function(x) {
return ((x - min(x)) / (max(x) - min(x)))
}

xTrain_norm <- c("")
xTrain_norm <- xTrain
for (i in 2:5) {
    xTrain_norm[, i] <- normalize(xTrain[, i])
}

xTest_norm <- c("")
xTest_norm <- xTest
for (i in 2:5) {
    xTest_norm[, i] <- normalize(xTest[, i])
}

xTrain_norm <-xTrain_norm %>%
  dummy_cols(select_columns = c("headA", "bodyA")) %>%
  select(-headA, -bodyA) %>%
  relocate(atRisk, .after = last_col()) %>%
  mutate(across(where(is.factor), unfactor)) %>%
  mutate(across(where(is.integer), as.numeric)) 

xTest_norm <-xTest_norm %>%
  dummy_cols(select_columns = c("headA", "bodyA")) %>%
  select(-headA, -bodyA) %>%
  relocate(atRisk, .after = last_col()) %>%
  mutate(across(where(is.factor), unfactor)) %>%
  mutate(across(where(is.integer), as.numeric)) 


# Create formula for factor variables depending on how many levels are used in data
xTrain_norm_formula <- c("")
for (i in 2:(ncol(xTrain_norm) - 1)) {
  if (i < (ncol(xTrain_norm) - 1)) { 
    xTrain_norm_formula <- paste0(xTrain_norm_formula, names(xTrain_norm[, i]), "+")
  } else {
      xTrain_norm_formula <- paste0(xTrain_norm_formula, names(xTrain_norm[, i]))
      xTrain_norm_formula <- paste0("atRisk~", xTrain_norm_formula)
  }
}

```

```{r ann, warning = FALSE, message = FALSE}
# modANN <- neuralnet(xTrain_norm_formula, xTrain_norm, hidden=2)
# 
# pred <-  neuralnet::compute(modANN, xTest_norm[, 2:ncol(xTest_norm)])
# pred$net.result
# table(pred$net.result[,1]>0.5,xTest_norm$atRisk)
# 
# plot(modANN)

# Best ANN Results after a few iterations 

# 85.41% accuracy with 1 hidden nodes
# 84.19% recall with 1 hidden nodes

# 85.41% accuracy with 2 hidden nodes
# 83.87% recall with 2 hidden nodes

# 70.00% accuracy with 3 hidden nodes
# 75.41% recall with 3 hidden nodes

# 83.79% accuracy with 4 hidden nodes
# 83.50% recall with 4 hidden nodes

# 28.08% accuracy with 5 hidden nodes
# 32.98% recall with 5 hidden nodes

```

```{r forest custom functions, message=FALSE, warning=FALSE}

# Tyler Reed
# CIS 635
# Winter 2021

rando_forest <- function(x,t,n,d) {
  # two stop statements to check for out of bounds for `n` and `d`
  if (n < 1 | n > nrow(x)) {
    stop("No. of instances chosen not within no. of rows of dataframe.")
  }
  if (d < 1 | d > ncol(x)) {
    stop("No. of attributes chosen not within no. of cols of dataframe.")
  }
  
  # for loop adding `rpart` models to list
  ls <- list()
  for (i in 1:t) {
    str <- "atRisk"
    sep <- "~"
    sam_d <- sample((ncol(x)), d, replace = FALSE)
    # for loop creating string of formula for sampling attributes
    for (j in sam_d) {
      str <- paste0(str, sep, names(x)[j])
      sep <- "+"
    }
    sam_n <- sample(nrow(x), n, replace = TRUE)
    ls[[i]] <- rpart(str, x[sam_n,])
  }
  return (ls)
}

pred <- function(ls, x) {
  # for loop creating dataframe of each models predictions per instance
  df <- cbind(tibble(predict(ls[[1]], x)))
  for (i in 2:length(ls)) {
    df <- cbind(df, data_frame(predict(ls[[i]], x)))
  }
  # for loop creating vector of ensemble random forest predictions by instance
  means <- as_tibble(rowMeans(df))
  vec <- c()
  for (i in 1:nrow(means)) {
    if (means[i, 1] <= 0.50) {
      vec <- rbind(vec, 0)
    } else if (means[i, 1] > 0.50) {
        vec <- rbind(vec, 1)
    }
  }
  return(vec)
}  
```



```{r random forest, warning = FALSE, message = FALSE}

xTrain_forest <- xTrain_noFactors %>%
  select(-id)

forest <- rando_forest(xTrain_forest, t = 10, n = 3000, d = 5)

pred_forest <- pred(forest, xTest_noFactors)
table(pred_forest, xTest_noFactors$atRisk)

# Best with several iterations
# 86.37% accuracy with  hidden nodes
# 87.35% recall with 4 hidden nodes


```

```{r k-means, warning = FALSE, message = FALSE}



```


